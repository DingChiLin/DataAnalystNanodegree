{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Enron Submission Free-Response Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? \n",
    "\n",
    "#### Background and Goal： \n",
    "\n",
    "Enron Corporation was one of the major energy company with several tens of thousands of employs and went bankrupt in 2001. Many important people in this company has been found guilty of fraudulent commercial activities. During the investigation, lots of confidential emails and financial details were released. \n",
    "\n",
    "For this project, the goal is using 146 financial data records which, for each of them, we known already whether they are person of interest (who have been proven guilty) or not to build a model that can predict if an unknown person is POI or not given the same financial features. Machine Learning is a way to build that model by writing a program to train the computer with the known data and generate a model to predict those unknowns.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "1. 146 data points\n",
    "2. 18 POI\n",
    "3. 21 features (including poi)\n",
    "4. 141 data points don't have 'loan_advances' value, 128 data points don't have 'director_fees' value and 127 data points don't have 'restricted_stock_deferred' value.\n",
    "\n",
    "#### Outliers:\n",
    "\n",
    "1. I remove the 'TOTAL' records since this is just the some of all other records.\n",
    "2. I remove 5 records with more than 18 'NaN' features and is not a POI, since I only have 20 features (including the email) in each record, if more the nine-tenths of them are unknown, I think it is not a good data to put in the analysis.\n",
    "\n",
    "After doing this, only 140 records remained\n",
    "\n",
    "Some records have very extreme values at some features like salary or total_payments, but I think it is not a good thing to remove them since this may be just the key feature of finding those POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "\n",
    "I create 2 new features by adding up all the 14 financial data ('salary', 'total_payments', 'total_stock_value', ... etc) to a new one called 'financial_info', and adding all the 5 communicating related data ('to_messages', 'from_this_person_to_poi', ... etc) to a another one called 'communicating_info'.\n",
    "\n",
    "I create other 2 new features by taking a natural log transformation of the two features above, called 'log_financial_info' and 'log_communicating_info' respectively,  since the distribution of these two data are highly right skewed, a log transform can make them distributed more like a normal distribution.\n",
    "\n",
    "I think this features may give me some general information about this two different kinds of data and have a more reasonable distribution.\n",
    "\n",
    "Finally, I use a pipeline in GridSearchCV that will firstly use StandardScaler to scale the data, then use SelectKBest to choose the best features, then use PCA to reduce the dimensions, and finally, test Learning Algorithm with different combination of parameters.\n",
    "\n",
    "The best result use all data in the SelectKBest step and reduce it to 4 component in the PCA step.\n",
    "\n",
    "The sorted score of all features are listed below:\n",
    "\n",
    "1. ('restricted_stock_deferred', 0.06709599078118228),\n",
    "2. ('from_messages', 0.18720023084121545),\n",
    "3. ('deferral_payments', 0.24851659019006084),\n",
    "4. ('to_messages', 1.4913280996536658),\n",
    "5. ('communicating_info', 1.6444298032226174),\n",
    "5. ('director_fees', 2.0050173086930219),\n",
    "7. ('from_this_person_to_poi', 2.2516565052501276),\n",
    "8. ('log_communicating_info', 3.9895509131051026),\n",
    "9. ('other', 4.0118881468420575),\n",
    "10. ('from_poi_to_this_person', 4.9405719627154756),\n",
    "11. ('expenses', 5.6765390190559293),\n",
    "12. ('loan_advances', 7.0080408695438878),\n",
    "13. ('log_financial_info', 8.1084785051533181),\n",
    "14. ('shared_receipt_with_poi', 8.1199380824959544),\n",
    "15. ('total_payments', 8.4722340090109469),\n",
    "16. ('restricted_stock', 8.8120939338279296),\n",
    "17. ('long_term_incentive', 9.4725171365744743),\n",
    "18. ('deferred_income', 11.047759080679354),\n",
    "19. ('financial_info', 16.398530700629301),\n",
    "20. ('salary', 17.432087382499649),\n",
    "21. ('bonus', 19.989875526651542),\n",
    "22. ('total_stock_value', 23.337370376732615),\n",
    "23. ('exercised_stock_options', 23.979597340395269)\n",
    "\n",
    "The explain ratio of the 4 PCA components are listed below:\n",
    "\n",
    "1. 0.34195999\n",
    "2. 0.18886029\n",
    "3. 0.07531905\n",
    "4. 0.06393028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?\n",
    "\n",
    "\n",
    "#### Algorithms I tried:\n",
    "GaussianNB, SVC, RandomForestClassifier, AdaBoost, LogisticRegression\n",
    "\n",
    "#### Final Decision:\n",
    "LogisticRegression\n",
    "\n",
    "#### Reason:\n",
    "\n",
    "According to Microsoft Azure's suggestion, I choose five algorithms that best fit my situation : Predicting\n",
    "two categories. Finally, I choose the one which gives me high recall score and acceptable precision score as the best algorithm.\n",
    "\n",
    "reference: http://download.microsoft.com/download/A/6/1/A613E11E-8F9C-424A-B99D-65344785C288/microsoft-machine-learning-algorithm-cheat-sheet-v6.pdf\n",
    "\n",
    "#### Result:\n",
    "\n",
    "| Algorithm  |  recall |  precision |\n",
    "|---|---|---|---|---|\n",
    "| GaussianNB  | 0.23  | 0.26  |\n",
    "| SVC  | 1.0  | 0.15  |\n",
    "| RandomForestClassifier  | 0.87  | 0.29  |\n",
    "| AdaBoost  | 0.19  | 0.26  |\n",
    "| LogisticRegression | 0.84 | 0.30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).\n",
    "\n",
    "The parameters in machine learning algorithms may change the stopping criteria or even the score evaluation methods. The performance of each algorithm may change dramatically using different parameters in different data sets. If I don't do this well, on one hand, the model may overfit the training data set and give a bad result at the testing data set or further unseen data sets, on the other hand, the model may stop too early and give a very weak model that performs badly at both the training data set and testing data set. Moreover, if I use the wrong score method, it may give me a model with high score but not the model which I want it to be.\n",
    "\n",
    "In my project, I use GridSearchCV to search lots of combinations of parameters. Since I want my model to find as many POIs as possible instead of just identify non-POIs, I give the recall score more weight. After some test and tuning, I create a customer score function that will give the weight of recall score three times larger than the weight of precision score to be the scoring function using in GridSearchCV\n",
    "\n",
    "I choose LogisticRegression as my final model and the best parameters is \n",
    "\n",
    "1. 'selection': {'k': 'all'}\n",
    "2. 'reducer': {'n_components': 4}\n",
    "3. 'classifier' :{'C': 0.001, 'tol': 1e-32, 'class_weight': {False: 1, True: 7.8}}. \n",
    "\n",
    "The best parameters of other models that I have tried are listed below:\n",
    "\n",
    "| Algorithm  |  selection |  reducer | classifier |\n",
    "|---|---|---|---|---|\n",
    "| naive_bayes  | {'k': 10}  | {'n_components': 10}  | -- |\n",
    "| SVC  | {'k': 'all'}  | {'n_components': 5}  | {'C': 1, 'gamma': 0.0005, 'class_weight': {False: 1, True: 9}}|\n",
    "| RandomForestClassifier  | {'k': 'all'}  | {'n_components': 4}  | {'max_features': 'sqrt', 'samples_split': 2, 'class_weight': {False: 1, True: 10}, 'max_depth': 1, 'criterion': 'gini'} |\n",
    "| AdaBoost  | {'k': 'all'}  | {'n_components': 1}  | {'learning_rate': 1, 'n_estimators': 50} |\n",
    "\n",
    "reference:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "\n",
    "Validation is the way to see how well your model perform on non-training data (or testing data). One of the classic mistakes is overfitting, which means that the model fit very well on the training data, but perform poorly on the other data. One of the ways to avoid this situation is using cross-validation to make sure the data has been trained and tested by different kinds of splitting many times, then we will have more confidence in this model if all the result is satisfactory.\n",
    "\n",
    "The cross validation algorithm I use in this project is StratifiedShuffleSplit. I choose StratifiedShuffleSplit because I don't have too much poi data in my dataset, I need to make sure that each fold contains roughly the same proportions of the two types of class labels. I choose test_size to be 0.3 and run 10 iteration for StratifiedShuffleSplit since it give me the best result in an acceptable running time.\n",
    "\n",
    "reference: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html\n",
    "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "#### Evalutaion Metrics\n",
    "\n",
    "Since I don't have many POIs in my dataset, if I using accuracy as my evaluation metric, it may just predict everybody as non-POI and still give me a great result (better than 0.8). So I choose recall and precision as my evalutation metric.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "Recall = TruePositive / (TruePositive + FalseNegative)\n",
    "\n",
    "By definition, a higher recall score would mean less false negatives. That is, if we want all the positive (POI) to be identified as positive by our model, we need to improve the recall value because it means reduce the false negative value (a person is POI but identified as non-POI)\n",
    "\n",
    "\n",
    "#### Precision\n",
    "\n",
    "Precision = TruePositive / (TruePositive + FalsePositive)\n",
    "\n",
    "By definition, a higher precision score would mean less false positive. If we want to improve the recall value, it will make our model more like to identify a person as POI than non-POI, which means, it may increase the false positive value (a person who is not a POI but identified as POI) at the same time. So I need to keep the precision value above some threshold, 0.3 in this case, to make sure our model doesn't just identify everybody as POI blindly. \n",
    "\n",
    "\n",
    "#### Final Result\n",
    "\n",
    "I choose recall as my major evaluation metric since I want to identify those few POI's as great as possible, which means, in the real world situation, that my model can help law enforcement officials find potential POIs. However, I don't want my model to have too poor precision value, which means, I don't want my model to predict too many non-POIs as POIs and waste related person too much time to clear out those innocent people.\n",
    "\n",
    "My final model gives me 0.85 recall score and 0.30 precision score both in my evaluation method mentioned above and evaluation method provided in tester.py. This result means my model can identify POI well and not gives too many false alarms just as I wished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
